# 플래닝
## 목표
- s3 최적화(cost optimization)

## s3 storage의 type
- standar
- ia
- glacier(빙하)

## intelligent tiering

라이프사이클을 적용할 수도 있다.  
일주일만에 IA 한달에 glacier가 되게 해! 도 가능하다.

## 분석을 위한 step
처음에 intelligent tiering을 적용하고 효과를 볼수 있어야 한다.  
common api가 필요하다.

## Target
- zigzag-main을 완성시키자는 목표로 시작
    - s3 list api 개발
        - 모두 가져올지 bucket별로 가져올지
        - bucket이 너무 크지 않다면 버킷별로 하는게 적당해 보인다.
        - option은 it가 아닌것
    - s3 modify api 개발
        - 오래된것은 삭제할껀지? 등 object list를 받아서 한꺼번에 변경할 수 있도록?? chunk로 나눠서 go routine으로 해볼까..?
    - s3 history api 개발
        - 변경 날자 시간, object명
        - 실패
    - 오로라 서버리스로 해보자
        - sql, sqlx, squirrel, gorm

## 주의사항
- 128KB 이하의 object는 intelligent tiering을 적용한후 접근이 없어도 FA구간에 머문다.
- 30일 기준으로 FA, IA, AA, DAA 로 이동하므로 30일 이전에 삭제될 object에 지정하기에 적합하지 않다.

## 질문
- frequent access
- infrequent access
- archive access
- deep archive access

총 네개 의 tier가 존재하는데 archive access 아래로는 비용이 비싸다.
1. 어디까지 적용할 것인지?(glacier까지 arhicive accesss까지 적용하는건 뒤로 미룬다.)


지라에 티켓을 만드는 일

- 트리거 만들기
- list api 개발
- list modify 개발
- history api개발
- rdb 셋팅
- crud 개발

화면개발은 뒤로 미루고 api오픈해놓고 읽을 수 있도록만

## 추후
- s3 object의 접근횟수를 확인할 수 있는지?
- 언제 intelligent tiering을 할지, 라이프사이클을 적용할지
- 최적의 케이스를 얻기위한 추가 로직이 필요하진 않을지

## NOSQL
```
s3로 스파크로 한번에 맞습니다.
오로라 서버리스
```

## RDB
```
aws rds proxy가 있다.
람다같은 애들은 커넥션 풀을 유지하기 쉽지 않다.
proxy를 띄워놓고 걔가 커넥션 풀을 유지해준다.
```